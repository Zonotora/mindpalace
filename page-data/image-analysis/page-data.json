{"componentChunkName":"component---src-templates-file-js","path":"/image-analysis","result":{"data":{"markdownRemark":{"html":"<h1 id=\"Image-processing\">Image processing</h1>\n<h2 id=\"Linear-filtering\">Linear filtering</h2>\n<h2 id=\"Non-linear-filtering\">Non-linear filtering</h2>\n<h1 id=\"Feature-detection\">Feature detection</h1>\n<h2 id=\"Feature-detectors\">Feature detectors</h2>\n<ol>\n<li>Point features</li>\n<li>Aperture problem</li>\n<li>spatially varying weighting (or window) function: <a href=\"https://en.wikipedia.org/wiki/Window_function\">https://en.wikipedia.org/wiki/Window_function</a></li>\n<li>auto-correlation function or surface</li>\n<li>Adaptive non-maximal suppression (ANMS).</li>\n<li>Measuring repeatability.</li>\n<li>Scale invariance</li>\n<li>SIFT</li>\n<li>\n<p>Rotational invariance and orientation estimation</p>\n<ul>\n<li>dominant orientation</li>\n</ul>\n</li>\n<li>aggregation window vs detection window</li>\n<li>Affine invariance</li>\n</ol>\n<h2 id=\"Feature-descriptors\">Feature descriptors</h2>\n<p>Feature descriptors are used to match keypoints retrieved by feature detection algorithms in different images. When we have extracted descriptors from each feature in at least two images, we begin by selecting a <strong>matching strategy</strong> (determined by the context, e.g. image stitching, object detecting, etc) that determines which correspondences that are qualified to the next stage for further processing. To perform matching as efficiently as possible the second step is to select performant <strong>data structures</strong> for the corresponding problem.</p>\n<ol>\n<li>Bias and gain normalization (MOPS).</li>\n<li>Scale invariant feature transform (SIFT) : <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">https://en.wikipedia.org/wiki/Scale-invariant_feature_transform</a></li>\n<li>PCA-SIFT.</li>\n<li>Gradient location-orientation histogram (GLOH).</li>\n<li>Steerable filters.</li>\n<li>\n<p>ROC curve</p>\n<ul>\n<li>the use of TP (true positives), FP (false positive), TN (true negatives), and FN (false negatives) when measuring the performance of the matching strategy.</li>\n</ul>\n</li>\n<li>Nearest Neighbor Distance Ratio (NNDR)</li>\n<li>\n<p>Efficient matching</p>\n<ul>\n<li>indexing structure can be used (hash maps)</li>\n<li>multi-dimensional hashing</li>\n<li>locality sensitive hashing</li>\n<li>parameter sensitive hashing</li>\n<li>k-d trees</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"Verification-of-matches\">Verification of matches</h2>\n<p>When we have gotten matches from the above steps we can use geometric alignment to verify <strong>inliers</strong> and <strong>outliers</strong> in the matches.</p>\n<ol>\n<li>Random sampling (RANSAC)</li>\n</ol>\n<h2 id=\"Feature-tracking\">Feature tracking</h2>\n<p>Instead of independently finding features in images and then match them with each other, we could find likely feature locations in the first image and search in these locations in the next images. Commonly used in video tracking applications.</p>\n<div class=\"reference-items\"></div>","frontmatter":{"slug":"/image-analysis","tags":[],"lastModified":"2022-01-18","created":"2022-01-18","title":"Image Analysis","header":[{"depth":1,"name":"Image processing","link":"Image-processing"},{"depth":2,"name":"Linear filtering","link":"Linear-filtering"},{"depth":2,"name":"Non-linear filtering","link":"Non-linear-filtering"},{"depth":1,"name":"Feature detection","link":"Feature-detection"},{"depth":2,"name":"Feature detectors","link":"Feature-detectors"},{"depth":2,"name":"Feature descriptors","link":"Feature-descriptors"},{"depth":2,"name":"Verification of matches","link":"Verification-of-matches"},{"depth":2,"name":"Feature tracking","link":"Feature-tracking"}]}}},"pageContext":{"slug":"/image-analysis"}},"staticQueryHashes":[]}