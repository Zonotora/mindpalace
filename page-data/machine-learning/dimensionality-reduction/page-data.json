{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/dimensionality-reduction","result":{"data":{"markdownRemark":{"html":"<p>In short, dimensionality reduction is reducing a high-dimensional dataset into a low-dimensional dataset. This is used for getting a better understanding of the data via visualization, reducing the need for storage, make the algorithms run faster and the learning easier. One famous technique of dimensionality reduction is principle component analysis. It is based on finding new basis vectors that are ordered in a way so that the first principle component has the highest priority (we see most of the variance in that direction). \n          <span class=\"keyword-link\" id=\"keyword-link-principle-component-analysis\">\n          Principle component analysis\n          </span>\n           (PCA) is implemented using the technique \n          <span class=\"keyword-link\" id=\"keyword-link-singular-value-decomposition\">\n          Singular value decomposition\n          </span>\n           (SVD). There is more efficient ways to implement this, and Scikit-learn uses a technique called \n          <span class=\"keyword-link\" id=\"keyword-link-low-rank-matrix-factorization\">\n          low-rank matrix factorization\n          </span>\n          . Other techniques involving SGD could be easier to implement in pratice than the low-rank matrix factorization.</p>\n<h1 id=\"t-SNE\">t-SNE</h1>\n<p>Measures similarities to locals points (local structure). \n          <sup><a class=\"reference-link\" href=\"#reference-link-tsnegoogle\">[1]</a></sup>\n         \n          <sup><a class=\"reference-link\" href=\"#reference-link-tsne\">[2]</a></sup>\n        </p>\n<h1 id=\"Applications\">Applications</h1>\n<p>Recommender systems like the ones used in Netflix, YouTube or Amazon for example.</p>\n<h2 id=\"Recommender-system\">Recommender system</h2>\n<p>Could be framed in different ways:</p>\n<ul>\n<li>binary classification</li>\n<li>regression</li>\n<li>ranking</li>\n</ul>\n<p>There are two main approaches for building recommender system:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">t</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">d</span></span></span></span>  recommend items that are similar to my selected items</li>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span> recommend items that are similar to my history</li>\n</ul>\n<h3 id=\"Colloborative-filtering\">Colloborative filtering</h3>\n<p>We can divide the feedback cycle into two categories:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span></span></span></span> could be some direct rating on some history item</li>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span></span></span></span> could be any kind of interaction between items and the user</li>\n</ul>\n<p>One problem with colloborative filtering is \n          <span class=\"keyword-link\" id=\"keyword-link-cold-start\">\n          cold start\n          </span>\n          . Matrix factorization in colloborative filtering is a bit different than when used in dimensionality reduction. Here it is used to predict and fill the missing cells. E.g. if we have user as a column vector and movies as a row vector we want to map each user to every movie with some probability of likeness.</p>\n<h2 id=\"Word-empeddings\">Word empeddings</h2>\n<p>Two high-level approaches to word empeddings:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">o</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span> we train the embeddings in with other parts of the model. The embeddings will be specialized for this task.</li>\n<li><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span> we train the embeddings in a more general sense so we can use them in other applications</li>\n</ul>\n<h3 id=\"Pre-training\">Pre-training</h3>\n<p><p>How can we pre-train word embeddings? In some sense, words that can be represented in similar contexts are assumed to have similar meanings and behave similarly. The model will thus build on this intuition. We could build on statistics, e.g. \n          <span class=\"keyword-link\" id=\"keyword-link-word-word-co-occurrence-matrix\">\n          word-word co-occurrence matrix\n          </span>\n          , and then use a matrix factorization step. GloVe \n          <sup><a class=\"reference-link\" href=\"#reference-link-glove\">[3]</a></sup>\n         is a famous matrix-based word embedding training method.</p></p>\n<h1 id=\"References\">References</h1>\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-tsnegoogle\">[1]</div><div class=\"reference-content\"><i>Visualizing Data Using t-SNE</i>. <a href=\"https://www.youtube.com/watch?v=RJVL80Gg3lA\">https://www.youtube.com/watch?v=RJVL80Gg3lA</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-tsne\">[2]</div><div class=\"reference-content\"><i>t-distributed stochastic neighbor embedding</i>. <a href=\"https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\">https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-glove\">[3]</div><div class=\"reference-content\"><a href=\"https://nlp.stanford.edu/projects/glove/\">https://nlp.stanford.edu/projects/glove/</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/dimensionality-reduction","tags":["machine-learning"],"lastModified":"2021-05-19","created":"2021-05-16","title":"Dimensionality Reduction","header":[{"depth":1,"name":"t-SNE","link":"t-SNE"},{"depth":1,"name":"Applications","link":"Applications"},{"depth":2,"name":"Recommender system","link":"Recommender-system"},{"depth":3,"name":"Colloborative filtering","link":"Colloborative-filtering"},{"depth":2,"name":"Word empeddings","link":"Word-empeddings"},{"depth":3,"name":"Pre-training","link":"Pre-training"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/dimensionality-reduction"}},"staticQueryHashes":[]}