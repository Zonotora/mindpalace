{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/feature-selection","result":{"data":{"markdownRemark":{"html":"<h1 id=\"Large-feature-sets\">Large feature sets</h1>\n<p>There are some problems arising with having a large set of features:</p>\n<ul>\n<li>there is a risk of information overload</li>\n<li>training time is increased</li>\n<li>more complexity will result in worse generalization</li>\n</ul>\n<h1 id=\"Types-of-selection\">Types of selection</h1>\n<p>So how do we choose the most useful and important features? The scikit-learn library has a number of different feature selection algorithms \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearn\">[1]</a></sup>\n        . There are three high-level approaches to feature selection.</p>\n<h2 id=\"Filter\">Filter</h2>\n<p>Is the input informative to the output? We want to apply some scoring functions to measure how well one feature perform in comparison to another. There are different scoring functions for classification and regression. One example of a scoring function is mutual information. It is defined like this</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.813113em;vertical-align:-1.386113em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0500050000000003em;\"><span style=\"top:-1.8999949999999999em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.386113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.050005em;\"><span style=\"top:-1.8999949999999999em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span></span></span></span><span style=\"top:-3.0500049999999996em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.250005em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>The mutual information function is thus telling us how closely linked the two random variables <span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span> and <span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> are. If they are independent from each other the mutual information function will produce a zero. Other scoring functions are derived from statistical tests.</p>\n<p>Given a scoring function we can select a subset in two different ways:</p>\n<ul>\n<li>selecting the top <span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> elements</li>\n<li>selecting the top 1 percentile</li>\n</ul>\n<p>See \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearn\">[1]</a></sup>\n         for more information.</p>\n<h3 id=\"Drawbacks\">Drawbacks</h3>\n<p>Filter-based methods are used in isolation. The filter-based scoring functions do not relate features with each other. Features could be useless if used by itself, but useful when combined with another feature. Filter-based methods will not notice this. Two features could be identical as well, and thus creating a redundance.</p>\n<h2 id=\"Wrapper\">Wrapper</h2>\n<p>Pseudo-python code of how the alogorithm work</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">S <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">while</span> <span class=\"token punctuation\">(</span>no improvement<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    best <span class=\"token operator\">=</span> find feature F that gives greatest improvement to S\n\n    <span class=\"token keyword\">if</span> best <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n        add F to S</code></pre></div>\n<p>Advantages compared to filter-based methods are that the machine learning model is used and involved in choosing features. We will not add any redundant features as well.</p>\n<h3 id=\"Drawbacks-1\">Drawbacks</h3>\n<p>There are three main drawbacks:</p>\n<ul>\n<li>the solution is suboptimal because the algorithm is greedy and will not find the optimal solution</li>\n<li>even though the algorithm is greedy it can be very costly if there are many features.</li>\n<li>it can be difficult to find features that are useful when grouped together.</li>\n</ul>\n<h2 id=\"Embedded\">Embedded</h2>\n<p>Embedded methods are defined by machine learning that have built-in feature selection properties. This is the main characteristic of the decision tree. You could say that decision trees have built-in filter methods (scoring system). Linear models could also have built-in feature selection.</p>\n<h1 id=\"Conclusion\">Conclusion</h1>\n<p>It is useful to</p>\n<ul>\n<li>get rid of redudant features</li>\n<li>consider feature selection as a part of the tuning process</li>\n</ul>\n<h1 id=\"References\">References</h1>\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearn\">[1]</div><div class=\"reference-content\"><i>Feature selection</i>. <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html\">https://scikit-learn.org/stable/modules/feature_selection.html</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/feature-selection","tags":[],"lastModified":"2021-04-09","created":"2021-04-08","title":"Feature Selection","header":[{"depth":1,"name":"Large feature sets","link":"Large-feature-sets"},{"depth":1,"name":"Types of selection","link":"Types-of-selection"},{"depth":2,"name":"Filter","link":"Filter"},{"depth":3,"name":"Drawbacks","link":"Drawbacks"},{"depth":2,"name":"Wrapper","link":"Wrapper"},{"depth":3,"name":"Drawbacks","link":"Drawbacks"},{"depth":2,"name":"Embedded","link":"Embedded"},{"depth":1,"name":"Conclusion","link":"Conclusion"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/feature-selection"}},"staticQueryHashes":[]}