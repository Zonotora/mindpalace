{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/hyperparameters","result":{"data":{"markdownRemark":{"html":"<h1 id=\"Definition\">Definition</h1>\n<p>Hyperparameters control how learning algorithms work. In a decision tree the maximum depth is one hyperparameter. In neural networks two examples are the number of layers and the size of layers. A model often needs extensive tuning in order to work properly.</p>\n<p>Scikit-learn provides some useful ways of determining hyperparameters \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearn\">[1]</a></sup>\n        .</p>\n<h1 id=\"Tuning\">Tuning</h1>\n<p>Tuning the hyperparameters could be very time-consuming and involve trial and error. There are however alternatives.</p>\n<h2 id=\"Grid-search\">Grid search</h2>\n<p>Grid search is one of them which is a brute force searching algorithm over a finite set of hyperparameter settings. All combinations are tested and the best is selected. This approach belongs to the more time-consuming ones. In a decision tree we could search for the best maximum tree depth.</p>\n<h2 id=\"Random-search\">Random search</h2>\n<p>Random search \n          <sup><a class=\"reference-link\" href=\"#reference-link-bergstra\">[2]</a></sup>\n         finds a set of hyperparameters with good performance a lot quicker than grid search. We define a distribution for each hyperparameter (thus we may need some understanding of how the hyperparameter work). We then iterate while picking samples randomly from each hyperparamter distribution.</p>\n<h2 id=\"Other\">Other</h2>\n<p>There are more sophisticated methods as well \n          <sup><a class=\"reference-link\" href=\"#reference-link-snoek\">[3]</a></sup>\n        .</p>\n<h1 id=\"References\">References</h1>\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearn\">[1]</div><div class=\"reference-content\"><i>Tuning the hyper-parameters of an estimator</i>. <a href=\"https://scikit-learn.org/stable/modules/grid_search.html\">https://scikit-learn.org/stable/modules/grid_search.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-bergstra\">[2]</div><div class=\"reference-content\">James Bergstra and Yoshua Bengio (2012). <i>Random Search for Hyper-Parameter Optimization</i>. <a href=\"https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-snoek\">[3]</div><div class=\"reference-content\">Jasper Snoek, Hugo Larochelle and Ryan P. Adams (2012). <i>PRACTICAL BAYESIAN OPTIMIZATION OF MACHINE LEARNING ALGORITHMS</i>. <a href=\"https://arxiv.org/pdf/1206.2944.pdf\">https://arxiv.org/pdf/1206.2944.pdf</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/hyperparameters","tags":["machine-learning"],"lastModified":"2021-04-09","created":"2021-04-08","title":"Hyperparameters","header":[{"depth":1,"name":"Definition","link":"Definition"},{"depth":1,"name":"Tuning","link":"Tuning"},{"depth":2,"name":"Grid search","link":"Grid-search"},{"depth":2,"name":"Random search","link":"Random-search"},{"depth":2,"name":"Other","link":"Other"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/hyperparameters"}},"staticQueryHashes":[]}