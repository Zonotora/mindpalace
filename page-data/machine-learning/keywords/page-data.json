{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/keywords","result":{"data":{"markdownRemark":{"html":"<ul>\n<li>generalization</li>\n<li>prediction</li>\n<li>training data</li>\n<li>induction</li>\n<li>test set</li>\n<li>regression</li>\n<li>\n<p>classification</p>\n<ul>\n<li>binary</li>\n<li>multiclass</li>\n</ul>\n</li>\n<li>ranking</li>\n<li>features</li>\n<li>labels</li>\n<li>histogram</li>\n<li>\n<p>loss function</p>\n<ul>\n<li>squared loss</li>\n<li>absolute loss</li>\n<li>zero/one loss</li>\n</ul>\n</li>\n<li>parity (function)</li>\n<li>noise</li>\n<li>supervised learning</li>\n<li>unsupervised learning</li>\n<li>reinforcement learning</li>\n<li>validation set</li>\n<li>information gain</li>\n<li>cross-validation</li>\n<li>support vector</li>\n<li>homogeneous</li>\n<li>committee</li>\n<li>weak learners</li>\n<li>boosting</li>\n<li>patch representation</li>\n<li>bag of words (text representation)</li>\n<li>shape representation</li>\n<li>meta features</li>\n<li>combinatorial transformations</li>\n<li>logarithmic transformation</li>\n<li>precision/recall metric</li>\n<li>accuracy metric</li>\n<li>f-measure</li>\n<li>sensitivity/specificity metric</li>\n<li>ROC curve</li>\n<li>AUC</li>\n<li>development data</li>\n<li>jack-knifing</li>\n<li>imbalanced data</li>\n<li>induced distribution</li>\n<li>feature selection <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">https://en.wikipedia.org/wiki/Feature_selection</a></li>\n<li>predictive model</li>\n<li>LASSO</li>\n<li>elastic net</li>\n<li>ridge regression</li>\n</ul>\n<h2 id=\"Bias\">Bias</h2>\n<h2 id=\"Bagging\">Bagging</h2>\n<p>Is also called bootstrap aggregation. It is a technique for reducing variances for estimated prediction functions and does so by averaging a number of noisy but approximately unbiased models.</p>\n<h2 id=\"Inductive-bias\">Inductive bias</h2>\n<p>The preference for one distinction over another. If the inductive bias is too far away from the concept that is being learned, the whole learning might fail.</p>\n<h2 id=\"Cross-validation\">Cross-validation</h2>\n<p>Is sometimes called rotation estimation which by itself describes the behaviour very well. For example, if you break up your training data into 10 equally big partitions, you use the learning alogorithm on 9 of them and test on the remaining partition for 10 cycles. Thus, every partition will be the training set once. One disadvantage is that it is computationally expensive.</p>\n<h2 id=\"Normalization\">Normalization</h2>\n<p>Is a good way of keeping the data consistent. There are two basic types of normalization: example and feature normalization.</p>\n<h3 id=\"Feature-normalization\">Feature normalization</h3>\n<p>Go through every feature and apply the same adjustment across all examples. There are two standard techniques to use: centering and scaling. Centering to keep the data set close around the origin. Scaling to make sure each feature has variance 1 across the training data.</p>\n<h3 id=\"Example-normalization\">Example normalization</h3>\n<p>Go through every feature but adjust them individually. The standard technique is to make sure that each feature vector has the length of one. The advantages of example normalization is that comparisons between data sets are more straightforward.</p>\n<h2 id=\"Approximation-error\">Approximation error</h2>\n<p>Will measure how well the model family is performing.</p>\n<h2 id=\"Estimation-error\">Estimation error</h2>\n<p>Will measure how far off one classifer is from the optimal classifier of that type.</p>\n<h2 id=\"Bias-and-variance-trade-off\">Bias and variance trade-off</h2>\n<p>The trade-off between approximation and estimation error is usually called the bias/variance trade-off. The bias corresponding to the approximation error and the variance corrsponding to the estimation error.</p>\n<h2 id=\"Imbalanced-data\">Imbalanced data</h2>\n<p>The imbalanced data problem refers to the problem where the distribution from which the data is taken has an imbalance. This is not good because machine learning algorithms will try to minimize the error, and thus, predict in favor of the imbalance majority. They can often achieve really good results by doing nothing. Hence, you probably not care about predicting accuracy.</p>\n<h2 id=\"Feature-selection\">Feature selection</h2>\n<h2 id=\"Embedded-methods\">Embedded methods</h2>\n<p>Embedded methods are used to learn which features best contribute to the learning of a model while it is being created. Common methods are regularization methods.</p>\n<h2 id=\"Regularization-methods\">Regularization methods</h2>\n<p>Regularization methods or penalization methods introduce additional constraints which makes the model bias toward fewer constraints.</p>\n<div classname=\"references\"></div>","frontmatter":{"slug":"/machine-learning/keywords","tags":[],"lastModified":"2021-04-05","created":"2021-03-22","title":"Keywords","header":[{"depth":2,"name":"Bias","link":"Bias"},{"depth":2,"name":"Bagging","link":"Bagging"},{"depth":2,"name":"Inductive bias","link":"Inductive-bias"},{"depth":2,"name":"Cross-validation","link":"Cross-validation"},{"depth":2,"name":"Normalization","link":"Normalization"},{"depth":3,"name":"Feature normalization","link":"Feature-normalization"},{"depth":3,"name":"Example normalization","link":"Example-normalization"},{"depth":2,"name":"Approximation error","link":"Approximation-error"},{"depth":2,"name":"Estimation error","link":"Estimation-error"},{"depth":2,"name":"Bias and variance trade-off","link":"Bias-and-variance-trade-off"},{"depth":2,"name":"Imbalanced data","link":"Imbalanced-data"},{"depth":2,"name":"Feature selection","link":"Feature-selection"},{"depth":2,"name":"Embedded methods","link":"Embedded-methods"},{"depth":2,"name":"Regularization methods","link":"Regularization-methods"}]}}},"pageContext":{"slug":"/machine-learning/keywords"}},"staticQueryHashes":[]}