{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/keywords","result":{"data":{"markdownRemark":{"html":"<ul>\n<li>generalization</li>\n<li>generalization gap</li>\n<li>prediction</li>\n<li>training data</li>\n<li>induction</li>\n<li>test set</li>\n<li>regression</li>\n<li>\n<p>classification</p>\n<ul>\n<li>binary</li>\n<li>multiclass</li>\n</ul>\n</li>\n<li>ranking</li>\n<li>features</li>\n<li>labels</li>\n<li>histogram</li>\n<li>\n<p>loss function</p>\n<ul>\n<li>squared loss</li>\n<li>absolute loss</li>\n<li>zero/one loss</li>\n</ul>\n</li>\n<li>parity (function)</li>\n<li>noise</li>\n<li>supervised learning</li>\n<li>unsupervised learning</li>\n<li>reinforcement learning</li>\n<li>semisupervised learning</li>\n<li>validation set</li>\n<li>information gain</li>\n<li>cross-validation</li>\n<li>gini score</li>\n<li>support vector</li>\n<li>homogeneous</li>\n<li>committee</li>\n<li>weak learners</li>\n<li>boosting</li>\n<li>patch representation</li>\n<li>bag of words (text representation)</li>\n<li>shape representation</li>\n<li>meta features</li>\n<li>combinatorial transformations</li>\n<li>logarithmic transformation</li>\n<li>precision/recall metric</li>\n<li>accuracy metric</li>\n<li>f-measure</li>\n<li>sensitivity/specificity metric</li>\n<li>ROC curve</li>\n<li>AUC score</li>\n<li>development data</li>\n<li>jack-knifing</li>\n<li>imbalanced data</li>\n<li>induced distribution</li>\n<li>feature selection <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">https://en.wikipedia.org/wiki/Feature_selection</a></li>\n<li>predictive model</li>\n<li>LASSO</li>\n<li>elastic net</li>\n<li>ridge regression</li>\n<li>one-hot encoding</li>\n<li>TF-IDF</li>\n<li>mutual information</li>\n<li>hyperparameters</li>\n<li>grid search</li>\n<li>black box optimization</li>\n<li>automated machine learning (AutoML)</li>\n<li>stacking</li>\n<li>shallow decision tree</li>\n<li>random forests</li>\n<li>hypothesis space</li>\n<li>linear classifiers</li>\n<li>linearly separable</li>\n<li>least-square regression</li>\n<li>widrow-hoff</li>\n<li>inter-annotator agreement</li>\n<li>chance-corrected agreement measure</li>\n<li>chance agreement probability</li>\n<li>objective function</li>\n<li>regularizer</li>\n<li>unconstrained optimization</li>\n<li>constrained optimization</li>\n<li>gradient</li>\n<li>batch</li>\n<li>early stopping</li>\n<li>logistic</li>\n<li>sigmoid</li>\n<li>tanh</li>\n<li>ReLu</li>\n<li>log odds</li>\n<li>likelihood function</li>\n<li>log loss</li>\n<li>maximum a posteriori</li>\n<li>Gaussian prior</li>\n<li>Laplace prior</li>\n<li>one-versus-rest</li>\n<li>one-versus-one</li>\n<li>softmax</li>\n<li>cross-entropy loss</li>\n<li>margin</li>\n<li>structural risk minimization theorem</li>\n<li>feedforward neural network</li>\n<li>multilayer perceptron</li>\n<li>input units/nodes</li>\n<li>hidden units/nodes</li>\n<li>output units/nodes</li>\n<li>activation</li>\n<li>universal approximation theorem</li>\n<li>minibatch</li>\n<li>\n<p>adaptive</p>\n<ul>\n<li>adam</li>\n<li>adagrad</li>\n<li>RMSProp</li>\n</ul>\n</li>\n<li>dropout</li>\n<li>data augmentation</li>\n<li>AdaBoost</li>\n<li>gradient boosting</li>\n<li>pseudo-residual</li>\n<li>residual</li>\n<li>learning rate</li>\n<li>ensemble size</li>\n<li>measure</li>\n<li>downstream task</li>\n<li>word error rate</li>\n<li>BLEU</li>\n<li>overlap-based metric</li>\n<li>humans-in-the-loop</li>\n<li>true positives</li>\n<li>false positives</li>\n<li>true negatives</li>\n<li>false negatives</li>\n<li>coefficient of determination</li>\n<li>confidence score</li>\n<li>search engine</li>\n<li>ranking systems</li>\n<li>precision at k</li>\n<li>scorer</li>\n<li>ranker</li>\n<li>feature extraction</li>\n<li>SIFT</li>\n<li>translational invariance</li>\n<li>spotting patterns</li>\n<li>convolutinal filters</li>\n<li>pooling</li>\n<li>fully connected layers</li>\n<li>dense layers</li>\n<li>redidual connections</li>\n<li>normalizations</li>\n<li>kernel</li>\n<li>feature map</li>\n<li>vanishing gradients</li>\n<li>exploding gradients</li>\n<li>mathematical instability</li>\n<li>batch normalization</li>\n<li>transfer learning</li>\n<li>freeze and unfreeze model</li>\n<li>fine-tune model</li>\n<li>catastrophic forgetting</li>\n<li>clustering (flat and hierachical)</li>\n<li>k-medoids</li>\n<li>mean shift</li>\n<li>Gaussian mixture</li>\n<li>DBSCAN</li>\n<li>agglomerative (clustering)</li>\n<li>divisive (clustering)</li>\n<li>evaluation (internal and external evaluation)</li>\n<li>silhoutte score</li>\n<li>purity score</li>\n<li>inverse purity score</li>\n<li>residual sum of squares</li>\n<li>NP-hard</li>\n<li>elbow method</li>\n<li>density-based clustering methods</li>\n<li>core point</li>\n<li>noise point or outlier</li>\n<li>matrix factorization</li>\n<li>low rank matrix factorization</li>\n</ul>\n<h1 id=\"Bias\">Bias</h1>\n<h1 id=\"Nonresponse-bias\">Nonresponse bias</h1>\n<h1 id=\"Stacking\">Stacking</h1>\n<h1 id=\"Bagging\">Bagging</h1>\n<p>Is also called bootstrap aggregation. It is a technique for reducing variances for estimated prediction functions and does so by averaging a number of noisy but approximately unbiased models.</p>\n<h1 id=\"Spinning\">Spinning</h1>\n<p>Based on the same idea as bagging and is also called feature bagging or random subspace learning.</p>\n<h1 id=\"Inductive-bias\">Inductive bias</h1>\n<p>The preference for one distinction over another. If the inductive bias is too far away from the concept that is being learned, the whole learning might fail.</p>\n<h1 id=\"Cross-validation\">Cross-validation</h1>\n<p>Is sometimes called rotation estimation which by itself describes the behaviour very well. For example, if you break up your training data into 10 equally big partitions, you use the learning alogorithm on 9 of them and test on the remaining partition for 10 cycles. Thus, every partition will be the training set once. One disadvantage is that it is computationally expensive.</p>\n<h1 id=\"Normalization\">Normalization</h1>\n<p>Is a good way of keeping the data consistent. There are two basic types of normalization: example and feature normalization.</p>\n<h2 id=\"Feature-normalization\">Feature normalization</h2>\n<p>Go through every feature and apply the same adjustment across all examples. There are two standard techniques to use: centering and scaling. Centering to keep the data set close around the origin. Scaling to make sure each feature has variance 1 across the training data.</p>\n<h2 id=\"Example-normalization\">Example normalization</h2>\n<p>Go through every feature but adjust them individually. The standard technique is to make sure that each feature vector has the length of one. The advantages of example normalization is that comparisons between data sets are more straightforward.</p>\n<h1 id=\"Approximation-error\">Approximation error</h1>\n<p>Will measure how well the model family is performing.</p>\n<h1 id=\"Estimation-error\">Estimation error</h1>\n<p>Will measure how far off one classifer is from the optimal classifier of that type.</p>\n<h1 id=\"Bias-variance-trade-off\">Bias-variance trade-off</h1>\n<p>The trade-off between approximation and estimation error is usually called the bias/variance trade-off. The bias corresponding to the approximation error and the variance corrsponding to the estimation error.</p>\n<h1 id=\"Imbalanced-data\">Imbalanced data</h1>\n<p>The imbalanced data problem refers to the problem where the distribution from which the data is taken has an imbalance. This is not good because machine learning algorithms will try to minimize the error, and thus, predict in favor of the imbalance majority. They can often achieve really good results by doing nothing. Hence, you probably not care about predicting accuracy.</p>\n<h1 id=\"Feature-selection\">Feature selection</h1>\n<h1 id=\"Embedded-methods\">Embedded methods</h1>\n<p>Embedded methods are used to learn which features best contribute to the learning of a model while it is being created. Common methods are regularization methods.</p>\n<h1 id=\"Regularization-methods\">Regularization methods</h1>\n<p>Regularization methods or penalization methods introduce additional constraints which makes the model bias toward fewer constraints.</p>\n<h1 id=\"Feature-imputation\">Feature imputation</h1>\n<p>It will try to fill any missing data. We could replace the missing value with a constant (e.g. the mean value), a random value or a prediction from the other values.</p>\n<h1 id=\"The-Widrow-Hoff-algorithm\">The Widrow-Hoff algorithm</h1>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">w <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># N epochs</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> the training <span class=\"token builtin\">set</span>\n        g <span class=\"token operator\">=</span> w <span class=\"token operator\">*</span> x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        error <span class=\"token operator\">=</span> g <span class=\"token operator\">-</span> y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        w <span class=\"token operator\">=</span> w <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> error <span class=\"token operator\">*</span> x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">return</span> w</code></pre></div>\n<h1 id=\"Crowdsourcing\">Crowdsourcing</h1>\n<p>A common technique for annotating data. It uses a large pool of non-expert annotators to annotate the data.</p>\n<h1 id=\"Deep-learning\">Deep learning</h1>\n<p>Deep learning is a neural network with many hidden layers. The universal approximation theorem states that one hidden layers should be enough to approximate any function, but it is often more practical to stack many hidden layers on each other.</p>\n<h1 id=\"Backpropagation\">Backpropagation</h1>\n<p>Backpropagation is the trick of using the gradients of the weights of layers occuring later in the hierarchy to compute the gradient when using the chain rule.</p>\n<h1 id=\"Intrinsic-evaluation\">Intrinsic evaluation</h1>\n<p>Intrinsic evaluation is the performance measured in isolation using some metric computed automatically.</p>\n<h1 id=\"Extrinsic-evaluation\">Extrinsic evaluation</h1>\n<p>How does one change to the predictor affect the performance?</p>\n<h1 id=\"F-score\">F-score</h1>\n<p>The F-score may refer to a clustering method or a classification method.</p>\n<h1 id=\"K-means\">K-means</h1>\n<p>K-means is probably the most popular technique for clustering and the idea behind it is to find a set of K clusters such that each data point is close to its centroid (mean vector).</p>\n<h1 id=\"Lloyds-algorithm\">Lloyd's algorithm</h1>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">while</span> clusters don't change<span class=\"token punctuation\">:</span>\n    insert x_i to cluster S_k\n    recompute cluster centroids <span class=\"token keyword\">for</span> each S_k\n<span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>S_1<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>S_k<span class=\"token punctuation\">]</span></code></pre></div>\n<h1 id=\"The-elbow-method\">The elbow method</h1>\n<p>When we use k-means for some clustering problem and want to choose the number of clusters we wish the algorithm should find, the elbow method presumes that there are some natural cluster optima. The loss function will drop quickly until we reach this optima, but increasing the numbers more will have diminishing returns. If we plot the number of clusters and the loss, we know we can apply the elbow method if the curve looks like an elbow.</p>\n<h1 id=\"Principle-component-analysis\">Principle component analysis</h1>\n<p>Principle component analysis (PCA)</p>\n<h1 id=\"Singular-value-decomposition\">Singular value decomposition</h1>\n<p>Singular value decomposition (SVD)</p>\n<h1 id=\"Low-rank-matrix-factorization\">Low-rank matrix factorization</h1>\n<p>A more space efficient technique for implementing PCA.</p>\n<h1 id=\"Cold-start\">Cold start</h1>\n<p>How to we handle new users and new items in colloborative filtering?</p>\n<h1 id=\"Word-embeddings\">Word embeddings</h1>\n<p>We represent words for NNs using a low-dimensional representation of real numbers.</p>\n<h1 id=\"Word-word-co-occurrence-matrix\">Word-word co-occurrence matrix</h1>\n<p>We count the occurrence of words occurring together.</p>\n<div class=\"reference-items\"></div>","frontmatter":{"slug":"/machine-learning/keywords","tags":["chalmers","machine-learning","keywords"],"lastModified":"2021-05-19","created":"2021-03-22","title":"Keywords","header":[{"depth":1,"name":"Bias","link":"Bias"},{"depth":1,"name":"Nonresponse bias","link":"Nonresponse-bias"},{"depth":1,"name":"Stacking","link":"Stacking"},{"depth":1,"name":"Bagging","link":"Bagging"},{"depth":1,"name":"Spinning","link":"Spinning"},{"depth":1,"name":"Inductive bias","link":"Inductive-bias"},{"depth":1,"name":"Cross-validation","link":"Cross-validation"},{"depth":1,"name":"Normalization","link":"Normalization"},{"depth":2,"name":"Feature normalization","link":"Feature-normalization"},{"depth":2,"name":"Example normalization","link":"Example-normalization"},{"depth":1,"name":"Approximation error","link":"Approximation-error"},{"depth":1,"name":"Estimation error","link":"Estimation-error"},{"depth":1,"name":"Bias-variance trade-off","link":"Bias-variance-trade-off"},{"depth":1,"name":"Imbalanced data","link":"Imbalanced-data"},{"depth":1,"name":"Feature selection","link":"Feature-selection"},{"depth":1,"name":"Embedded methods","link":"Embedded-methods"},{"depth":1,"name":"Regularization methods","link":"Regularization-methods"},{"depth":1,"name":"Feature imputation","link":"Feature-imputation"},{"depth":1,"name":"The Widrow-Hoff algorithm","link":"The-Widrow-Hoff-algorithm"},{"depth":1,"name":"Crowdsourcing","link":"Crowdsourcing"},{"depth":1,"name":"Deep learning","link":"Deep-learning"},{"depth":1,"name":"Backpropagation","link":"Backpropagation"},{"depth":1,"name":"Intrinsic evaluation","link":"Intrinsic-evaluation"},{"depth":1,"name":"Extrinsic evaluation","link":"Extrinsic-evaluation"},{"depth":1,"name":"F-score","link":"F-score"},{"depth":1,"name":"K-means","link":"K-means"},{"depth":1,"name":"Lloyd's algorithm","link":"Lloyd's-algorithm"},{"depth":1,"name":"The elbow method","link":"The-elbow-method"},{"depth":1,"name":"Principle component analysis","link":"Principle-component-analysis"},{"depth":1,"name":"Singular value decomposition","link":"Singular-value-decomposition"},{"depth":1,"name":"Low-rank matrix factorization","link":"Low-rank-matrix-factorization"},{"depth":1,"name":"Cold start","link":"Cold-start"},{"depth":1,"name":"Word embeddings","link":"Word-embeddings"},{"depth":1,"name":"Word-word co-occurrence matrix","link":"Word-word-co-occurrence-matrix"}]}}},"pageContext":{"slug":"/machine-learning/keywords"}},"staticQueryHashes":[]}