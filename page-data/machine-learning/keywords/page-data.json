{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/keywords","result":{"data":{"markdownRemark":{"html":"<ul>\n<li>generalization</li>\n<li>prediction</li>\n<li>training data</li>\n<li>induction</li>\n<li>test set</li>\n<li>regression</li>\n<li>\n<p>classification</p>\n<ul>\n<li>binary</li>\n<li>multiclass</li>\n</ul>\n</li>\n<li>ranking</li>\n<li>features</li>\n<li>labels</li>\n<li>histogram</li>\n<li>\n<p>loss function</p>\n<ul>\n<li>squared loss</li>\n<li>absolute loss</li>\n<li>zero/one loss</li>\n</ul>\n</li>\n<li>parity (function)</li>\n<li>noise</li>\n<li>supervised learning</li>\n<li>unsupervised learning</li>\n<li>reinforcement learning</li>\n<li>validation set</li>\n<li>information gain</li>\n<li>cross-validation</li>\n<li>support vector</li>\n<li>homogeneous</li>\n<li>committee</li>\n<li>weak learners</li>\n<li>boosting</li>\n<li>patch representation</li>\n<li>bag of words (text representation)</li>\n<li>shape representation</li>\n<li>meta features</li>\n<li>combinatorial transformations</li>\n<li>logarithmic transformation</li>\n<li>precision/recall metric</li>\n<li>accuracy metric</li>\n<li>f-measure</li>\n<li>sensitivity/specificity metric</li>\n<li>ROC curve</li>\n<li>AUC</li>\n</ul>\n<h2 id=\"Bias\">Bias</h2>\n<h2 id=\"Bagging\">Bagging</h2>\n<p>Is also called bootstrap aggregation. It is a technique for reducing variances for estimated prediction functions and does so by averaging a number of noisy but approximately unbiased models.</p>\n<h2 id=\"Inductive-bias\">Inductive bias</h2>\n<p>The preference for one distinction over another. If the inductive bias is too far away from the concept that is being learned, the whole learning might fail.</p>\n<h2 id=\"Cross-validation\">Cross-validation</h2>\n<p>Is sometimes called rotation estimation which by itself describes the behaviour very well.</p>\n<h2 id=\"Normalization\">Normalization</h2>\n<p>Is a good way of keeping the data consistent. There are two basic types of normalization: example and feature normalization.</p>\n<h3 id=\"Feature-normalization\">Feature normalization</h3>\n<p>Go through every feature and apply the same adjustment across all examples. There are two standard techniques to use: centering and scaling. Centering to keep the data set close around the origin. Scaling to make sure each feature has variance 1 across the training data.</p>\n<h3 id=\"Example-normalization\">Example normalization</h3>\n<p>Go through every feature but adjust them individually. The standard technique is to make sure that each feature vector has the length of one. The advantages of example normalization is that comparisons between data sets are more straightforward.</p>","frontmatter":{"slug":"/machine-learning/keywords","tags":[],"lastModified":"2021-04-03","created":"2021-03-22","title":"Keywords","header":[{"depth":2,"name":"Bias","link":"Bias"},{"depth":2,"name":"Bagging","link":"Bagging"},{"depth":2,"name":"Inductive bias","link":"Inductive-bias"},{"depth":2,"name":"Cross-validation","link":"Cross-validation"},{"depth":2,"name":"Normalization","link":"Normalization"},{"depth":3,"name":"Feature normalization","link":"Feature-normalization"},{"depth":3,"name":"Example normalization","link":"Example-normalization"}]}}},"pageContext":{"slug":"/machine-learning/keywords"}},"staticQueryHashes":[]}