{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/keywords","result":{"data":{"markdownRemark":{"html":"<h1 id=\"Generalization\">Generalization</h1>\n<p>In machine learning generalization refers to how well a trained model is to classify unseen data.</p>\n<h1 id=\"Generalization-gap\">Generalization gap</h1>\n<p>Generalization gap is defined as the difference between the model's performance on training data versus unseen data from the same distribution.</p>\n<h1 id=\"Prediction\">Prediction</h1>\n<p>Predictions are machine learning model's way of mapping an input to an output.</p>\n<h1 id=\"Training-data\">Training data</h1>\n<p>A training set is collected from a distribution very similar to how it will look when the model is put into practive. You usually split the collected data into training data and test data with the majority of that being the training data.</p>\n<h1 id=\"Validation-set\">Validation set</h1>\n<p>The model is evaluated on the validation set, which aims to be unbiased in comparison to the training set, while optimizing the model's hyperparameters. The validation set will become biased eventually because we tune the model to get as good performance as possible on that set.</p>\n<h1 id=\"Test-set\">Test set</h1>\n<p>The test set is the final evaluation of the model and provides an unbiased final evaluation of the model. After a model is evaluated on the test set it should not be further optimzed because that will introduce bias.</p>\n<h1 id=\"Induction\">Induction</h1>\n<p>Induction in machine learning is the process of inferring general rules from specific</p>\n<h1 id=\"Regression\">Regression</h1>\n<p>A machine learning model for regression tries to map inputs to outputs in the continuous space instead of the discrete space which is used in classification.</p>\n<h1 id=\"Classification\">Classification</h1>\n<p>A machine learning model for classification tries to identify which set of categories an observer belongs to. Thus mapping an input to and output in the discrete space. Binary classification is a special of multiclass classification where there is only two groups.</p>\n<h1 id=\"Ranking\">Ranking</h1>\n<p>It is a type of application of typically supervised, semi-supervised of reinforcement learning wherein the training data has some partial order between each item. The order is often induced by a numerical or ordinal score.</p>\n<h1 id=\"Feature\">Feature</h1>\n<p>Within machine learning features are some individual measurable properties of a phenomenon. Features could be numberic, but also structural information as strings or graphs. Together they are used to build patterns which the machine learning model learns.</p>\n<h1 id=\"Label\">Label</h1>\n<p>In supervised or semi-supervised learning, a label is the corresponding output in the training data.</p>\n<h1 id=\"Loss-function\">Loss function</h1>\n<p>A loss function is formally a function that maps an event or some values to a real number with a cost associated with it, and the optimization algorithm tries to minimize that cost. There are different loss functions depending on the type of model. In regression problems the most common loss functions are \n          <sup><a class=\"reference-link\" href=\"#reference-link-lossreg\">[1]</a></sup>\n        :</p>\n<ul>\n<li>Mean absolute error (MAE)</li>\n<li>Mean absolute percentage error (MAPE)</li>\n<li>Mean squared error (MSE)</li>\n<li>Root mean squared error (RMSE)</li>\n<li>Huber loss</li>\n<li>Log-cosh loss</li>\n</ul>\n<p>For classification the most common loss functions are \n          <sup><a class=\"reference-link\" href=\"#reference-link-lossclass\">[2]</a></sup>\n        :</p>\n<ul>\n<li>Cross-entropy loss (or log loss)</li>\n<li>Hinge loss</li>\n<li>Squared hinge loss</li>\n</ul>\n<h1 id=\"Parity\">Parity</h1>\n<p>A parity function is a boolean function whose values are 1 if the input vector has an off number of ones, else 0.</p>\n<h1 id=\"Noise\">Noise</h1>\n<p>Noise in machine learning can be a wanted property or not depending on the problem. It may increse the complexity of the model and the time of learning which may degrade the performance. However, it may as well help generalize the model as in data augmentation.</p>\n<h1 id=\"Supervised-learning\">Supervised learning</h1>\n<p>Supervised learning is one of the major learning paradigms in machine learning. It requires that the training data is labeled. Thus a model tries to imitate by examples.</p>\n<h1 id=\"Unsupervised-learning\">Unsupervised learning</h1>\n<p>Unsupervised learning is one of the major learning paradigms in machine learning. Unlike supervised machine learning, unsupervised learning does not have any labeled data but must instead discover certain patterns about the data itself.</p>\n<h1 id=\"Semi-supervised-learning\">Semi-supervised learning</h1>\n<p>Semi-supervised learning could be looked at as a mixture of supervised learning and unsupervised learning, that combines a small amount of labeled data with a large amount of unlabeled data during the training phase.</p>\n<h1 id=\"Reinforcement-learning\">Reinforcement learning</h1>\n<p>Reinforcement learning is one of the major learning paradigms in machine learning. It concerns how an agent should take actions in a defined enviroment in order to maximize the cumulative reward. The reward function is here the objective function.</p>\n<h1 id=\"Cross-validation\">Cross-validation</h1>\n<p>Cross-validation is a technique that is used to reduce the number of samples required for the training process of a model. The technique removes the need of a validation set. Instead one basic approach called k-fold cross-validation splits the training set into k smaller sets called folds. Each of these folds act like the validation set in turns for a total of k times. After cross-validation the model is evaulated on the test set as usual. The technique can be computationally heavy.</p>\n<h1 id=\"Decision-tree\">Decision tree</h1>\n<p>A decision tree is a model that orders its weights in tree-based format where leaves represent class labels and branches represent conjuntions of features that lead to the different decisions. It is the branches that are updated in the training process. Decisions trees could be used in regression as well wherin the leaves represent a condition and the the branches usually correspond to yes or no given the validity of the condition. Decision trees are one of the more popular machine learning models because of their simplicity and interpretablility.</p>\n<h2 id=\"Entropy\">Entropy</h2>\n<p>Entropy is the measure of disorder, a measure of purity or homogeneity. Thus, it could be seen as how random the data points are in a distribution. Greater disorder results in lower impurity.</p>\n<h2 id=\"Information-gain\">Information gain</h2>\n<p>Entropy plays an important role in information gain. Information gain is known in information theory as the amount of information gained about a random variable from observing another random variable. In the context of decision trees it is a good measure for deciding whether a feature has relevance, although it is not perfect \n          <sup><a class=\"reference-link\" href=\"#reference-link-wikiinformationgain\">[3]</a></sup>\n        .</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\"><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.20696799999999996em;\"><span style=\"top:-2.4558600000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24414em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<h2 id=\"Gini-score\">Gini score</h2>\n<p>As in information gain entropy plays an important role here as well in determining how pure a set of data points are. It ranges between 0-1 where 0 expresses purity, namely, all data points belong to the same class, whereas 1 indicates a random distribution amoong the data points.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">i</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span>\n<h1 id=\"Ensemble\">Ensemble</h1>\n<p>An ensemble method take advantage of multiple learning algorithms to obtain a better predictive result that could not otherwise be obtained.</p>\n<h2 id=\"Boosting\">Boosting</h2>\n<p>Boosting is an ensemble technique (meta-algorithm) that involves an incremental build of the ensemble model by training each new model instance in a fashion that will \"correct\" how earlier instances misclassified the data. Boosting has been shown to yield better results than bagging but also tends to overfit to a higher degree. It is used to reduce bias and variance. It converts weak learners to strong learners.</p>\n<h3 id=\"AdaBoost\">AdaBoost</h3>\n<p>AdaBoost is short for Adaptive Boosting. It focuses on misclassified instances by previous classifiers and tweaks new weak learners to slightly better performance. As long as the performane of each weak learner is better than random guessing even by a little, the final model can be proven to converge to a strong learner. It has no loss function.</p>\n<h3 id=\"Gradient-boosting\">Gradient boosting</h3>\n<p>Like any other boosting method gradient boosting builds the model by stage-wise improvements of weak learners, but generalizes the model with an arbitrary differentiable loss function. What differs the AdaBoost algorithm from gradient boosting is that is does not have a loss function.</p>\n<h2 id=\"Stacking\">Stacking</h2>\n<p>Stacking is an ensemble technique (meta-algorithm) that combines several different learning algorithms to one united. Every other learning algorithm is trained on the available data, and then combined with a combiner algorithm to make the final prediction using the predictions of all the other algorithms as input.</p>\n<h2 id=\"Bagging\">Bagging</h2>\n<p>Bagging or bootstrap aggregation is an ensemble technique (meta-algorithm) that promotes total model variance by having each submodel vote with equal weight. Thus, it is a technique for reducing variances for estimated prediction functions and does so by averaging a number of noisy but approximately unbiased models. Bagging trains each model in the ensemble with a randomly chosen subset of the training set. The samples in bagging are different from each other but replacements are allowed which means that one instance may occur in several samples or none at all.</p>\n<h3 id=\"Random-forest\">Random forest</h3>\n<p>Random forests are an ensemble learning algorithm based on bagging and decision trees. The performance of multiple decision trees are thus combined and usually gives a better performance than one decision tree alone.</p>\n<h2 id=\"Spinning\">Spinning</h2>\n<p>Based on the same idea as bagging and is also called feature bagging or random subspace learning.</p>\n<h1 id=\"Weak-learner\">Weak learner</h1>\n<p>A weak learner is a classifier that is only partially correlated with the true classification. It performs better than random guesses but not very much.</p>\n<h1 id=\"Strong-learner\">Strong learner</h1>\n<p>A strong learner is a classifier that is very well correlated with the true classification.</p>\n<h1 id=\"Linear-models\">Linear models</h1>\n<p>Linear models base its prediction on linear functions as the name suggests.</p>\n<ul>\n<li>support vector machine</li>\n<li>linear classifiers</li>\n<li>logistic regression</li>\n<li>lasso</li>\n<li>elastic net</li>\n<li>ridge regression</li>\n<li>committee</li>\n<li>patch representation</li>\n<li>bag of words (text representation)</li>\n<li>shape representation</li>\n<li>meta features</li>\n<li>meta algorithm</li>\n<li>combinatorial transformations</li>\n<li>logarithmic transformation</li>\n<li>precision/recall metric</li>\n<li>accuracy metric</li>\n<li>f-measure</li>\n<li>sensitivity/specificity metric</li>\n<li>ROC curve</li>\n<li>AUC score</li>\n<li>development data</li>\n<li>jack-knifing</li>\n<li>imbalanced data</li>\n<li>induced distribution</li>\n<li>feature selection <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">https://en.wikipedia.org/wiki/Feature_selection</a></li>\n<li>predictive model</li>\n<li>one-hot encoding</li>\n<li>TF-IDF</li>\n<li>mutual information</li>\n<li>hyperparameters</li>\n<li>grid search</li>\n<li>black box optimization</li>\n<li>automated machine learning (AutoML)</li>\n<li>shallow decision tree</li>\n<li>hypothesis space</li>\n<li>linearly separable</li>\n<li>least-square regression</li>\n<li>inter-annotator agreement</li>\n<li>chance-corrected agreement measure</li>\n<li>chance agreement probability</li>\n<li>objective function</li>\n<li>regularizer</li>\n<li>unconstrained optimization</li>\n<li>constrained optimization</li>\n<li>gradient</li>\n<li>batch</li>\n<li>early stopping</li>\n<li>logistic</li>\n<li>sigmoid</li>\n<li>tanh</li>\n<li>ReLu</li>\n<li>log odds</li>\n<li>likelihood function</li>\n<li>log loss</li>\n<li>maximum a posteriori</li>\n<li>Gaussian prior</li>\n<li>Laplace prior</li>\n<li>one-versus-rest</li>\n<li>one-versus-one</li>\n<li>softmax</li>\n<li>cross-entropy loss</li>\n<li>margin</li>\n<li>structural risk minimization theorem</li>\n<li>input units/nodes</li>\n<li>hidden units/nodes</li>\n<li>output units/nodes</li>\n<li>activation</li>\n<li>universal approximation theorem</li>\n<li>minibatch</li>\n<li>\n<p>adaptive</p>\n<ul>\n<li>adam</li>\n<li>adagrad</li>\n<li>RMSProp</li>\n</ul>\n</li>\n<li>dropout</li>\n<li>data augmentation</li>\n<li>pseudo-residual</li>\n<li>residual</li>\n<li>learning rate</li>\n<li>ensemble size</li>\n<li>measure</li>\n<li>downstream task</li>\n<li>word error rate</li>\n<li>BLEU</li>\n<li>overlap-based metric</li>\n<li>humans-in-the-loop</li>\n<li>true positives</li>\n<li>false positives</li>\n<li>true negatives</li>\n<li>false negatives</li>\n<li>coefficient of determination</li>\n<li>confidence score</li>\n<li>search engine</li>\n<li>ranking systems</li>\n<li>precision at k</li>\n<li>scorer</li>\n<li>ranker</li>\n<li>feature extraction</li>\n<li>SIFT</li>\n<li>translational invariance</li>\n<li>spotting patterns</li>\n<li>convolutinal filters</li>\n<li>pooling</li>\n<li>fully connected layers</li>\n<li>dense layers</li>\n<li>redidual connections</li>\n<li>normalizations</li>\n<li>kernel</li>\n<li>feature map</li>\n<li>vanishing gradients</li>\n<li>exploding gradients</li>\n<li>mathematical instability</li>\n<li>batch normalization</li>\n<li>transfer learning</li>\n<li>freeze and unfreeze model</li>\n<li>fine-tune model</li>\n<li>catastrophic forgetting</li>\n<li>clustering (flat and hierachical)</li>\n<li>k-medoids</li>\n<li>mean shift</li>\n<li>Gaussian mixture</li>\n<li>DBSCAN</li>\n<li>agglomerative (clustering)</li>\n<li>divisive (clustering)</li>\n<li>evaluation (internal and external evaluation)</li>\n<li>silhoutte score</li>\n<li>purity score</li>\n<li>inverse purity score</li>\n<li>residual sum of squares</li>\n<li>NP-hard</li>\n<li>elbow method</li>\n<li>density-based clustering methods</li>\n<li>core point</li>\n<li>noise point or outlier</li>\n<li>matrix factorization</li>\n<li>low rank matrix factorization</li>\n<li>autoregressive model (time series)</li>\n<li>exogenous (ARX)</li>\n<li>sequence-to-sequence</li>\n<li>attention model</li>\n<li>transformer (the BERT model)</li>\n<li>induction</li>\n<li>histogram</li>\n<li>\n<p>loss function</p>\n<ul>\n<li>squared loss</li>\n<li>absolute loss</li>\n<li>zero/one loss</li>\n</ul>\n</li>\n<li>spinning</li>\n<li>exploration-exploitation dilemma</li>\n</ul>\n<h1 id=\"Bias\">Bias</h1>\n<h1 id=\"Nonresponse-bias\">Nonresponse bias</h1>\n<h1 id=\"Inductive-bias\">Inductive bias</h1>\n<p>The preference for one distinction over another. If the inductive bias is too far away from the concept that is being learned, the whole learning might fail.</p>\n<h1 id=\"Normalization\">Normalization</h1>\n<p>Is a good way of keeping the data consistent. There are two basic types of normalization: example and feature normalization.</p>\n<h2 id=\"Feature-normalization\">Feature normalization</h2>\n<p>Go through every feature and apply the same adjustment across all examples. There are two standard techniques to use: centering and scaling. Centering to keep the data set close around the origin. Scaling to make sure each feature has variance 1 across the training data.</p>\n<h2 id=\"Example-normalization\">Example normalization</h2>\n<p>Go through every feature but adjust them individually. The standard technique is to make sure that each feature vector has the length of one. The advantages of example normalization is that comparisons between data sets are more straightforward.</p>\n<h1 id=\"Approximation-error\">Approximation error</h1>\n<p>Will measure how well the model family is performing.</p>\n<h1 id=\"Estimation-error\">Estimation error</h1>\n<p>Will measure how far off one classifer is from the optimal classifier of that type.</p>\n<h1 id=\"Bias-variance-trade-off\">Bias-variance trade-off</h1>\n<p>The trade-off between approximation and estimation error is usually called the bias/variance trade-off. The bias corresponding to the approximation error and the variance corrsponding to the estimation error.</p>\n<h1 id=\"Imbalanced-data\">Imbalanced data</h1>\n<p>The imbalanced data problem refers to the problem where the distribution from which the data is taken has an imbalance. This is not good because machine learning algorithms will try to minimize the error, and thus, predict in favor of the imbalance majority. They can often achieve really good results by doing nothing. Hence, you probably not care about predicting accuracy.</p>\n<h1 id=\"Feature-selection\">Feature selection</h1>\n<h1 id=\"Embedded-methods\">Embedded methods</h1>\n<p>Embedded methods are used to learn which features best contribute to the learning of a model while it is being created. Common methods are regularization methods.</p>\n<h1 id=\"Regularization-methods\">Regularization methods</h1>\n<p>Regularization methods or penalization methods introduce additional constraints which makes the model bias toward fewer constraints.</p>\n<h1 id=\"Feature-imputation\">Feature imputation</h1>\n<p>It will try to fill any missing data. We could replace the missing value with a constant (e.g. the mean value), a random value or a prediction from the other values.</p>\n<h1 id=\"The-Widrow-Hoff-algorithm\">The Widrow-Hoff algorithm</h1>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">w <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># N epochs</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> the training <span class=\"token builtin\">set</span>\n        g <span class=\"token operator\">=</span> w <span class=\"token operator\">*</span> x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        error <span class=\"token operator\">=</span> g <span class=\"token operator\">-</span> y<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        w <span class=\"token operator\">=</span> w <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> error <span class=\"token operator\">*</span> x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">return</span> w</code></pre></div>\n<h1 id=\"Crowdsourcing\">Crowdsourcing</h1>\n<p>A common technique for annotating data. It uses a large pool of non-expert annotators to annotate the data.</p>\n<h1 id=\"Deep-learning\">Deep learning</h1>\n<p>Deep learning is a neural network with many hidden layers. The universal approximation theorem states that one hidden layers should be enough to approximate any function, but it is often more practical to stack many hidden layers on each other.</p>\n<h1 id=\"Backpropagation\">Backpropagation</h1>\n<p>Backpropagation is the trick of using the gradients of the weights of layers occuring later in the hierarchy to compute the gradient when using the chain rule.</p>\n<h1 id=\"Intrinsic-evaluation\">Intrinsic evaluation</h1>\n<p>Intrinsic evaluation is the performance measured in isolation using some metric computed automatically.</p>\n<h1 id=\"Extrinsic-evaluation\">Extrinsic evaluation</h1>\n<p>How does one change to the predictor affect the performance?</p>\n<h1 id=\"F-score\">F-score</h1>\n<p>The F-score may refer to a clustering method or a classification method.</p>\n<h1 id=\"K-means\">K-means</h1>\n<p>K-means is probably the most popular technique for clustering and the idea behind it is to find a set of K clusters such that each data point is close to its centroid (mean vector).</p>\n<h1 id=\"Lloyds-algorithm\">Lloyd's algorithm</h1>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">while</span> clusters don't change<span class=\"token punctuation\">:</span>\n    insert x_i to cluster S_k\n    recompute cluster centroids <span class=\"token keyword\">for</span> each S_k\n<span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>S_1<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>S_k<span class=\"token punctuation\">]</span></code></pre></div>\n<h1 id=\"The-elbow-method\">The elbow method</h1>\n<p>When we use k-means for some clustering problem and want to choose the number of clusters we wish the algorithm should find, the elbow method presumes that there are some natural cluster optima. The loss function will drop quickly until we reach this optima, but increasing the numbers more will have diminishing returns. If we plot the number of clusters and the loss, we know we can apply the elbow method if the curve looks like an elbow.</p>\n<h1 id=\"Principle-component-analysis\">Principle component analysis</h1>\n<p>Principle component analysis (PCA)</p>\n<h1 id=\"Singular-value-decomposition\">Singular value decomposition</h1>\n<p>Singular value decomposition (SVD)</p>\n<h1 id=\"Low-rank-matrix-factorization\">Low-rank matrix factorization</h1>\n<p>A more space efficient technique for implementing PCA.</p>\n<h1 id=\"Cold-start\">Cold start</h1>\n<p>How to we handle new users and new items in colloborative filtering?</p>\n<h1 id=\"Word-embeddings\">Word embeddings</h1>\n<p>We represent words for NNs using a low-dimensional representation of real numbers.</p>\n<h1 id=\"Word-word-co-occurrence-matrix\">Word-word co-occurrence matrix</h1>\n<p>We count the occurrence of words occurring together.</p>\n<h1 id=\"Reduction\">Reduction</h1>\n<p>Reduction in machine learning means that we convert a complictated problem into a bunch of simpler problems.</p>\n<h1 id=\"Part-of-speech-tagging\">Part-of-speech tagging</h1>\n<p>Input a sequence of word tokens and output a sequence of grammatical tags corresponding to each token.</p>\n<h1 id=\"Imitation-learning\">Imitation learning</h1>\n<p>A paradigm in machine learning where the model tries to imitate an \"expert\".</p>\n<h1 id=\"Feedforward-neural-network\">Feedforward neural network</h1>\n<p>Consists of connected layers of \"classifiers\" where intermediate classifiers are called hidden units and the final classifier is called output unit. Each hidden unit is computed by</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0001em;vertical-align:-0.2501em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span><span class=\"fix\"></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.050187499999999996em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2501em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span class=\"fix\"></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.050187499999999996em;\"></span><span class=\"mord mathnormal\">x</span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>and the output is computed by</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span><span class=\"fix\"></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.050187499999999996em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">o</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mord mathnormal\">h</span></span></span><span class=\"fix\"></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.050187499999999996em;\"></span><span class=\"mord mathnormal\">h</span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span> is the activation function.</p>\n<h1 id=\"Multilayer-perceptron\">Multilayer perceptron</h1>\n<p>See <a href=\"#Feedforward-neural-network\">Feedforward neural network</a>.</p>\n<h1 id=\"Recurrent-neural-networks\">Recurrent neural networks</h1>\n<p>They use states to represent previous events. After each step the state vector is recalculated.</p>\n<h1 id=\"References\">References</h1>\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-lossreg\">[1]</div><div class=\"reference-content\"><a href=\"https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6\">https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-lossclass\">[2]</div><div class=\"reference-content\"><a href=\"https://medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-classification-model-931cbf564d42\">https://medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-classification-model-931cbf564d42</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-wikiinformationgain\">[3]</div><div class=\"reference-content\"><a href=\"https://en.wikipedia.org/wiki/Information_gain_in_decision_trees#Drawbacks\">https://en.wikipedia.org/wiki/Information_gain_in_decision_trees#Drawbacks</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/keywords","tags":["keywords"],"lastModified":"2021-10-05","created":"2021-03-22","title":"Keywords","header":[{"depth":1,"name":"Generalization","link":"Generalization"},{"depth":1,"name":"Generalization gap","link":"Generalization-gap"},{"depth":1,"name":"Prediction","link":"Prediction"},{"depth":1,"name":"Training data","link":"Training-data"},{"depth":1,"name":"Validation set","link":"Validation-set"},{"depth":1,"name":"Test set","link":"Test-set"},{"depth":1,"name":"Induction","link":"Induction"},{"depth":1,"name":"Regression","link":"Regression"},{"depth":1,"name":"Classification","link":"Classification"},{"depth":1,"name":"Ranking","link":"Ranking"},{"depth":1,"name":"Feature","link":"Feature"},{"depth":1,"name":"Label","link":"Label"},{"depth":1,"name":"Loss function","link":"Loss-function"},{"depth":1,"name":"Parity","link":"Parity"},{"depth":1,"name":"Noise","link":"Noise"},{"depth":1,"name":"Supervised learning","link":"Supervised-learning"},{"depth":1,"name":"Unsupervised learning","link":"Unsupervised-learning"},{"depth":1,"name":"Semi-supervised learning","link":"Semi-supervised-learning"},{"depth":1,"name":"Reinforcement learning","link":"Reinforcement-learning"},{"depth":1,"name":"Cross-validation","link":"Cross-validation"},{"depth":1,"name":"Decision tree","link":"Decision-tree"},{"depth":2,"name":"Entropy","link":"Entropy"},{"depth":2,"name":"Information gain","link":"Information-gain"},{"depth":2,"name":"Gini score","link":"Gini-score"},{"depth":1,"name":"Ensemble","link":"Ensemble"},{"depth":2,"name":"Boosting","link":"Boosting"},{"depth":3,"name":"AdaBoost","link":"AdaBoost"},{"depth":3,"name":"Gradient boosting","link":"Gradient-boosting"},{"depth":2,"name":"Stacking","link":"Stacking"},{"depth":2,"name":"Bagging","link":"Bagging"},{"depth":3,"name":"Random forest","link":"Random-forest"},{"depth":2,"name":"Spinning","link":"Spinning"},{"depth":1,"name":"Weak learner","link":"Weak-learner"},{"depth":1,"name":"Strong learner","link":"Strong-learner"},{"depth":1,"name":"Linear models","link":"Linear-models"},{"depth":1,"name":"Bias","link":"Bias"},{"depth":1,"name":"Nonresponse bias","link":"Nonresponse-bias"},{"depth":1,"name":"Inductive bias","link":"Inductive-bias"},{"depth":1,"name":"Normalization","link":"Normalization"},{"depth":2,"name":"Feature normalization","link":"Feature-normalization"},{"depth":2,"name":"Example normalization","link":"Example-normalization"},{"depth":1,"name":"Approximation error","link":"Approximation-error"},{"depth":1,"name":"Estimation error","link":"Estimation-error"},{"depth":1,"name":"Bias-variance trade-off","link":"Bias-variance-trade-off"},{"depth":1,"name":"Imbalanced data","link":"Imbalanced-data"},{"depth":1,"name":"Feature selection","link":"Feature-selection"},{"depth":1,"name":"Embedded methods","link":"Embedded-methods"},{"depth":1,"name":"Regularization methods","link":"Regularization-methods"},{"depth":1,"name":"Feature imputation","link":"Feature-imputation"},{"depth":1,"name":"The Widrow-Hoff algorithm","link":"The-Widrow-Hoff-algorithm"},{"depth":1,"name":"Crowdsourcing","link":"Crowdsourcing"},{"depth":1,"name":"Deep learning","link":"Deep-learning"},{"depth":1,"name":"Backpropagation","link":"Backpropagation"},{"depth":1,"name":"Intrinsic evaluation","link":"Intrinsic-evaluation"},{"depth":1,"name":"Extrinsic evaluation","link":"Extrinsic-evaluation"},{"depth":1,"name":"F-score","link":"F-score"},{"depth":1,"name":"K-means","link":"K-means"},{"depth":1,"name":"Lloyd's algorithm","link":"Lloyd's-algorithm"},{"depth":1,"name":"The elbow method","link":"The-elbow-method"},{"depth":1,"name":"Principle component analysis","link":"Principle-component-analysis"},{"depth":1,"name":"Singular value decomposition","link":"Singular-value-decomposition"},{"depth":1,"name":"Low-rank matrix factorization","link":"Low-rank-matrix-factorization"},{"depth":1,"name":"Cold start","link":"Cold-start"},{"depth":1,"name":"Word embeddings","link":"Word-embeddings"},{"depth":1,"name":"Word-word co-occurrence matrix","link":"Word-word-co-occurrence-matrix"},{"depth":1,"name":"Reduction","link":"Reduction"},{"depth":1,"name":"Part-of-speech tagging","link":"Part-of-speech-tagging"},{"depth":1,"name":"Imitation learning","link":"Imitation-learning"},{"depth":1,"name":"Feedforward neural network","link":"Feedforward-neural-network"},{"depth":1,"name":"Multilayer perceptron","link":"Multilayer-perceptron"},{"depth":1,"name":"Recurrent neural networks","link":"Recurrent-neural-networks"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/keywords"}},"staticQueryHashes":[]}