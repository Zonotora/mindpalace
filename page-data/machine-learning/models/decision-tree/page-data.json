{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/models/decision-tree","result":{"data":{"markdownRemark":{"html":"<h1 id=\"What-is-a-decision-tree\">What is a decision tree?</h1>\n<p>Decision tree \n          <sup><a class=\"reference-link\" href=\"#reference-link-wiki\">[1]</a></sup>\n         is a classic model of learning and well suited for binary classification. The decision tree is made up of guesses where each node represents a guess and the path to each node the binary decision. Each non-terminal node has two children which corresponds to answering \"no\" or \"yes\". The questions can be seen as features and the rating is called the label. Decision trees can output probabilities as well.</p>\n<p>Scikit-learn has decision tree models \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearn\">[2]</a></sup>\n         \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearnc\">[3]</a></sup>\n         \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearnr\">[4]</a></sup>\n        .</p>\n<h1 id=\"The-learning-algorithm\">The learning algorithm</h1>\n<p>The learning algorithm can be simplified to:</p>\n<ul>\n<li>select the best feature F that corresponds to the best split (creating subsets)</li>\n<li>create a (sub)tree with F as the root</li>\n<li>repeat</li>\n</ul>\n<p>To not get an infinite recursion loop we have some base cases like checking similarity in each subset, checking max depth, etc.</p>\n<h1 id=\"Best-split\">Best split</h1>\n<p>It is best to split each set into homogeneous (little varaiation) subsets. There are a couple of techniques for determining the homogeneity of one set, for classification the most popular are: information gain or the gini score and for regression we can consider the variance (a subset with small variance).</p>\n<h1 id=\"Drawbacks\">Drawbacks</h1>\n<p>Decision trees tend to overfit and thus produce bad generalizations. They are usually not useful on their own because of this. However when used in ensembles e.g. in the popular model random forests they perform well.</p>\n<h1 id=\"References\">References</h1>\n\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-wiki\">[1]</div><div class=\"reference-content\"><i>Decision tree</i>. <a href=\"https://en.wikipedia.org/wiki/Decision_tree\">https://en.wikipedia.org/wiki/Decision_tree</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearn\">[2]</div><div class=\"reference-content\"><i>Decision Trees</i>. <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearnc\">[3]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearnr\">[4]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/models/decision-tree","tags":["chalmers","machine-learning"],"lastModified":"2021-04-09","created":"2021-03-21","title":"Decision Tree","header":[{"depth":1,"name":"What is a decision tree?","link":"What-is-a-decision-tree?"},{"depth":1,"name":"The learning algorithm","link":"The-learning-algorithm"},{"depth":1,"name":"Best split","link":"Best-split"},{"depth":1,"name":"Drawbacks","link":"Drawbacks"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/models/decision-tree"}},"staticQueryHashes":[]}