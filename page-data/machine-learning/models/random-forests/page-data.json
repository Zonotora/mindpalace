{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/models/random-forests","result":{"data":{"markdownRemark":{"html":"<h1 id=\"What-is-random-forests\">What is random forests?</h1>\n<p><p>Random forests \n          <sup><a class=\"reference-link\" href=\"#reference-link-wiki\">[1]</a></sup>\n         have very similar performance compared to boosting on many problems, but they are easier to train and tune. Trees are usually noisy which is why \n          <span class=\"keyword-link\" id=\"keyword-link-bagging\">\n          bagging\n          </span>\n           comes in handy. Each tree is identically distributed in bagging. The \n          <span class=\"keyword-link\" id=\"keyword-link-bias\">\n          bias\n          </span>\n           of the bagged trees will because of that have the same value as the individual trees.</p></p>\n<p>Random forests main idea is to improve the variance reduction by reducing the correlation between the trees it is built upon. By letting trees grow through a process of random selection of the input variables this can be achieved.</p>\n<p>Random forests and tree based models in general have little need for feature normalization. Random forests often work well without complicated setup and provides more robust results than a single decision tree. They work very well for tabulated data but not images, signals or text. They can be computationally heavy depending on how many different trees they are made up of, and are obviously more computationally heavy than a single decision tree. They are as well not as easy to interpret as single decision trees.</p>\n<p>Scikit-learn has a random forest classifier \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearnc\">[2]</a></sup>\n         and a random forest regressor \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearnr\">[3]</a></sup>\n        .</p>\n<h1 id=\"The-learning-algorithm\">The learning algorithm</h1>\n<p>Each tree in the ensemble is trained on its own training set through the technique bagging. Instead of consider all the possible features of the data set we only consider a subset, typically <span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.24em;vertical-align:-0.30499999999999994em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.935em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span><span style=\"top:-2.8950000000000005em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30499999999999994em;\"><span></span></span></span></span></span></span></span></span>, when we are building a tree branch.</p>\n<h1 id=\"Prediction\">Prediction</h1>\n<p>For regression we use the average value as the output.</p>\n<p>For classification we use voting or averaging of the probabilities as output.</p>\n<h1 id=\"Hyperparameters\">Hyperparameters</h1>\n<p>We can choose how many trees we want our model to consist of. More trees result in a slower model but more accurate because we take advantage of the fundamental idea of ensembles in relation to decision trees. We can also choose how many features we should consider when we build new tree nodes. The standard hyperparameters for decision trees apply here as well.</p>\n<h1 id=\"References\">References</h1>\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-wiki\">[1]</div><div class=\"reference-content\"><i>Random forest</i>. <a href=\"https://en.wikipedia.org/wiki/Random_forest\">https://en.wikipedia.org/wiki/Random_forest</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearnc\">[2]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearnr\">[3]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/models/random-forests","tags":["chalmers","machine-learning"],"lastModified":"2021-04-09","created":"2021-04-02","title":"Random Forests","header":[{"depth":1,"name":"What is random forests?","link":"What-is-random-forests?"},{"depth":1,"name":"The learning algorithm","link":"The-learning-algorithm"},{"depth":1,"name":"Prediction","link":"Prediction"},{"depth":1,"name":"Hyperparameters","link":"Hyperparameters"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/models/random-forests"}},"staticQueryHashes":[]}