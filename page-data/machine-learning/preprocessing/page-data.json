{"componentChunkName":"component---src-templates-file-js","path":"/machine-learning/preprocessing","result":{"data":{"markdownRemark":{"html":"<h1 id=\"Feature-preprocessing\">Feature preprocessing</h1>\n<p>An important concept of machine learning like many other problems there is some sort of \"garbage in, garbage out\". Machine learning may seems to magically classify data and solve all sorts of problems but if the data you put in to a machine learning model are out of context you can expect the model to perform out of context as well. One example is to have a very easy feature representation of an image (100x100) where you put every pixel into a 30.000 dimensional vector (three values for every pixel). In doing so we lose all locality information. It is therefore an important aspect to consider when designing learning models, i.e. how robust they are to noisy features. Redundant features is not good either (two features are redudant if they are highly correlated). It is as well important to observe that when the training sample N is small the chance of getting seeminly correlated data is increased even though the data are independent.</p>\n<p>Generally we need to do some preprocessing for categorical values in a data set, because almost every library uses numerical values. One-hot encoding is a very common way of dealing with categorical values and converting them to numerical ones. A direct integer encoding tends to work poorly in machine learning algorithms. We would give mathematical properties to each value even though there are none. It would create an unnecessary ordering which may mislead the machine learning algorithm.</p>\n<p>Magnitude differences can be a problem, but that depends on which machine learning algorithm that is used. It usually <strong><em>strongly</em></strong> affects many models like linear models,  neural networks, models based on distance or similarity (kNN, SVC, etc...). However, tree-based predictors will ignore any scaling differences (they will adapt to the new scaling), because they consider thresholds of one feature at a time.</p>\n<p>Scikit-learn provides a number of ways to preprocess the data \n          <sup><a class=\"reference-link\" href=\"#reference-link-sklearn\">[1]</a></sup>\n        .</p>\n<h2 id=\"Pruning\">Pruning</h2>\n<p>The principle of pruning is as follows, if you have a binary feature that only appears a small number of times <span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> you can simply remove them from consideration. You have to be careful to not overuse the technique of pruning because before we end up with no interesting data. \n          <span class=\"keyword-link\" id=\"keyword-link-normalization\">\n          Normalization\n          </span>\n           is also important.</p>\n<h1 id=\"Scaling-and-normalization\">Scaling and normalization</h1>\n<h2 id=\"Min-Max-scaling\">Min-Max scaling</h2>\n<p>Sqeeze the scale so that it is between zero and one.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714399999999998em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">x</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804400000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<h2 id=\"Standard-scaling\">Standard scaling</h2>\n<p>Use mean value and standard deviation to scale. After this transformation the mean value will be zero and the standard deviation will be one.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.45188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.57144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord overline\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.89444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span><span style=\"top:-3.81444em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"overline-line\" style=\"border-bottom-width:0.04em;\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804400000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<h2 id=\"Length-normalization\">Length normalization</h2>\n<p>The length of the feature vector will be one after this transformation.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0435600000000003em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.10756em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">∣</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<h2 id=\"Other-transformations\">Other transformations</h2>\n<p>We may try logarithms, square root, etc...</p>\n<h1 id=\"Missing-values\">Missing values</h1>\n<p>If there are missing values in some data row or column, we could just remove those instances. This is considered the brute force approach and will work fine if we have a lot of data. A more sophisticated method is \n          <span class=\"keyword-link\" id=\"keyword-link-feature-imputation\">\n          feature imputation\n          </span>\n          .</p>\n<h1 id=\"Evaluating\">Evaluating</h1>\n<p>It is very fair to say that achieving a high accuracy for a model is you want in most cases, however, in some cases it is better to let a little \"bad\" data through. For spotting problems (X versus not-X) there are better success with a metrics of precision/recall instead of accuracy for this reason. Thus, having a metric producing confidence is probably better in these cases.</p>\n<h1 id=\"Debugging\">Debugging</h1>\n<ul>\n<li>Generalization of test data</li>\n<li>Train/test data mismatch</li>\n<li>The learning algorithm</li>\n<li>Adequate representation</li>\n<li>Enough data</li>\n</ul>\n<h1 id=\"References\">References</h1>\n\n\n\n<div class=\"reference-items\"><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-sklearn\">[1]</div><div class=\"reference-content\"><i>Preprocessing data</i>. <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">https://scikit-learn.org/stable/modules/preprocessing.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-stdscalar\">[2]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a>. </div></div><div class=\"reference\"><div class=\"reference-number\"\n      id=\"reference-link-minmaxscalar\">[3]</div><div class=\"reference-content\"><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</a>. </div></div></div>","frontmatter":{"slug":"/machine-learning/preprocessing","tags":["chalmers","machine-learning"],"lastModified":"2021-04-30","created":"2021-04-02","title":"Preprocessing","header":[{"depth":1,"name":"Feature preprocessing","link":"Feature-preprocessing"},{"depth":2,"name":"Pruning","link":"Pruning"},{"depth":1,"name":"Scaling and normalization","link":"Scaling-and-normalization"},{"depth":2,"name":"Min-Max scaling","link":"Min-Max-scaling"},{"depth":2,"name":"Standard scaling","link":"Standard-scaling"},{"depth":2,"name":"Length normalization","link":"Length-normalization"},{"depth":2,"name":"Other transformations","link":"Other-transformations"},{"depth":1,"name":"Missing values","link":"Missing-values"},{"depth":1,"name":"Evaluating","link":"Evaluating"},{"depth":1,"name":"Debugging","link":"Debugging"},{"depth":1,"name":"References","link":"References"}]}}},"pageContext":{"slug":"/machine-learning/preprocessing"}},"staticQueryHashes":[]}